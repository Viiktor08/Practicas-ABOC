# PrÃ¡ctica 1: OptimizaciÃ³n sin Restricciones (ADALINA)

Este repositorio contiene la implementaciÃ³n del modelo **ADALINA (Adaptive Linear Neuron)** como soluciÃ³n a un problema de regresiÃ³n lineal mediante dos mÃ©todos de optimizaciÃ³n: **Descenso de Gradiente** y **MÃ©todo de Newton**.

## ğŸ“Œ DescripciÃ³n

El objetivo de esta prÃ¡ctica es minimizar el error cuadrÃ¡tico entre las predicciones de ADALINA y los valores reales del conjunto de datos **Boston Housing**. Se ha realizado:

- Preprocesamiento del dataset.
- ImplementaciÃ³n de ADALINA con:
  - Descenso de Gradiente (con cÃ¡lculo exacto del paso Ã³ptimo).
  - MÃ©todo de Newton (utilizando gradiente y matriz Hessiana).
- ComparaciÃ³n de ambos mÃ©todos en cuanto a iteraciones y precisiÃ³n.

## ğŸ“ Estructura del Proyecto

```
â”œâ”€â”€ ADALINAGradiente.py         # ImplementaciÃ³n con Descenso de Gradiente
â”œâ”€â”€ ADALINANewton.py            # ImplementaciÃ³n con MÃ©todo de Newton
â”œâ”€â”€ hou_all.csv                 # Dataset de viviendas (Boston Housing)
â”œâ”€â”€ P1_ABOC.pdf                 # Informe con resultados y conclusiones
```

## âš™ï¸ Requisitos

- Python 3.x
- LibrerÃ­as:
  - numpy
  - pandas

InstalaciÃ³n de dependencias:
```bash
pip install numpy pandas
```

## ğŸ§ª Instrucciones de EjecuciÃ³n

1. **Preprocesamiento**: el dataset `hou_all.csv` es cargado y normalizado internamente por ambos scripts.
2. **EjecuciÃ³n**:
   - Para el descenso de gradiente:
     ```bash
     python ADALINAGradiente.py
     ```
   - Para el mÃ©todo de Newton:
     ```bash
     python ADALINANewton.py
     ```

## ğŸ“Š Resultados

SegÃºn el informe:

| MÃ©todo               | Iteraciones hasta convergencia |
|----------------------|-------------------------------|
| Descenso de Gradiente| 320                           |
| MÃ©todo de Newton     | 1                             |

Ambos mÃ©todos producen predicciones similares, pero el MÃ©todo de Newton converge mucho mÃ¡s rÃ¡pido debido al uso de derivadas de segundo orden.

## ğŸ§  Conclusiones

- El **Descenso de Gradiente** es mÃ¡s sencillo y computacionalmente barato, pero converge lentamente.
- El **MÃ©todo de Newton** es mÃ¡s eficiente en tÃ©rminos de iteraciones gracias a su convergencia cuadrÃ¡tica, aunque requiere mayor capacidad computacional por el cÃ¡lculo de la matriz Hessiana.

## ğŸ‘¤ Autor

- VÃ­ctor Tirado  
- victortiradoarregui@uma.es  
- Universidad de MÃ¡laga â€” Algoritmos de BÃºsqueda y OptimizaciÃ³n Computacional

## ğŸ“„ Licencia

Este proyecto es solo con fines acadÃ©micos.
